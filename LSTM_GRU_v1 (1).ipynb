{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd"],"metadata":{"id":"jnwSYb4eyMhE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A4rRNFixyaS2","outputId":"39d323c7-c76b-497c-fafe-5b69d93fbd1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["data_path = '/content/drive/MyDrive/Predicting business cycle/Data/Financial_Indicator.csv'\n","output_path = '/content/drive/MyDrive/Predicting business cycle/Results/'"],"metadata":{"id":"sJItJbmuySyt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1Kowaemxep7"},"outputs":[],"source":["data_original = pd.read_csv(data_path)\n","\n","# Rename the column\n","data_original.rename(columns={'Unnamed: 0': 'Date'}, inplace=True)"]},{"cell_type":"code","source":["#### Do LSTM/GRU with one month time step as well (the current one is 3)"],"metadata":{"id":"vwJo3D4Z6GR1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_original.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"sohekw6yyRbQ","outputId":"bb2859ce-175a-44ab-e823-997fe592ad20"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         Date   INDPRO      GDP      SP500  UNRATE  FEDFUNDS  AWHMAN  UMCSENT  \\\n","0  1960-01-01  24.1712  542.648  58.029000     5.2      3.99    40.6    100.0   \n","1  1960-02-01  23.9561  542.648  55.775000     4.8      3.97    40.3    100.0   \n","2  1960-03-01  23.7410  542.648  55.015217     5.4      3.84    40.0     93.3   \n","3  1960-04-01  23.5528  541.080  55.700000     5.2      3.92    40.0     93.3   \n","4  1960-05-01  23.5259  541.080  55.215238     5.1      3.85    40.1     93.3   \n","\n","   TB3SMFFM  T10YFFM  CPALTT01USM657N  UEMPMEAN  PERMIT  business_cycle  \n","0      0.36     0.73        -0.340136      13.5  1092.0             1.0  \n","1     -0.01     0.52         0.341297      13.1  1088.0             1.0  \n","2     -0.53     0.41         0.000000      13.0   955.0             1.0  \n","3     -0.69     0.36         0.340136      12.6  1016.0             1.0  \n","4     -0.56     0.50         0.000000      11.9  1052.0             0.0  "],"text/html":["\n","  <div id=\"df-38093723-20b8-4555-808b-b20b22120c4f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>INDPRO</th>\n","      <th>GDP</th>\n","      <th>SP500</th>\n","      <th>UNRATE</th>\n","      <th>FEDFUNDS</th>\n","      <th>AWHMAN</th>\n","      <th>UMCSENT</th>\n","      <th>TB3SMFFM</th>\n","      <th>T10YFFM</th>\n","      <th>CPALTT01USM657N</th>\n","      <th>UEMPMEAN</th>\n","      <th>PERMIT</th>\n","      <th>business_cycle</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1960-01-01</td>\n","      <td>24.1712</td>\n","      <td>542.648</td>\n","      <td>58.029000</td>\n","      <td>5.2</td>\n","      <td>3.99</td>\n","      <td>40.6</td>\n","      <td>100.0</td>\n","      <td>0.36</td>\n","      <td>0.73</td>\n","      <td>-0.340136</td>\n","      <td>13.5</td>\n","      <td>1092.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1960-02-01</td>\n","      <td>23.9561</td>\n","      <td>542.648</td>\n","      <td>55.775000</td>\n","      <td>4.8</td>\n","      <td>3.97</td>\n","      <td>40.3</td>\n","      <td>100.0</td>\n","      <td>-0.01</td>\n","      <td>0.52</td>\n","      <td>0.341297</td>\n","      <td>13.1</td>\n","      <td>1088.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1960-03-01</td>\n","      <td>23.7410</td>\n","      <td>542.648</td>\n","      <td>55.015217</td>\n","      <td>5.4</td>\n","      <td>3.84</td>\n","      <td>40.0</td>\n","      <td>93.3</td>\n","      <td>-0.53</td>\n","      <td>0.41</td>\n","      <td>0.000000</td>\n","      <td>13.0</td>\n","      <td>955.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1960-04-01</td>\n","      <td>23.5528</td>\n","      <td>541.080</td>\n","      <td>55.700000</td>\n","      <td>5.2</td>\n","      <td>3.92</td>\n","      <td>40.0</td>\n","      <td>93.3</td>\n","      <td>-0.69</td>\n","      <td>0.36</td>\n","      <td>0.340136</td>\n","      <td>12.6</td>\n","      <td>1016.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1960-05-01</td>\n","      <td>23.5259</td>\n","      <td>541.080</td>\n","      <td>55.215238</td>\n","      <td>5.1</td>\n","      <td>3.85</td>\n","      <td>40.1</td>\n","      <td>93.3</td>\n","      <td>-0.56</td>\n","      <td>0.50</td>\n","      <td>0.000000</td>\n","      <td>11.9</td>\n","      <td>1052.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38093723-20b8-4555-808b-b20b22120c4f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-38093723-20b8-4555-808b-b20b22120c4f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-38093723-20b8-4555-808b-b20b22120c4f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-1c57cc2d-222b-443c-ae72-0f6fed5dacd3\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1c57cc2d-222b-443c-ae72-0f6fed5dacd3')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-1c57cc2d-222b-443c-ae72-0f6fed5dacd3 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data_original","summary":"{\n  \"name\": \"data_original\",\n  \"rows\": 768,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 768,\n        \"samples\": [\n          \"2015-09-01\",\n          \"1987-01-01\",\n          \"2012-01-01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"INDPRO\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 26.31620037668309,\n        \"min\": 22.1009,\n        \"max\": 104.1181,\n        \"num_unique_values\": 764,\n        \"samples\": [\n          62.2108,\n          49.8181,\n          103.0707\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GDP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7374.620364966031,\n        \"min\": 540.197,\n        \"max\": 27956.998,\n        \"num_unique_values\": 256,\n        \"samples\": [\n          19280.084,\n          567.664,\n          2723.883\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SP500\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1079.1392762788457,\n        \"min\": 53.743333544049946,\n        \"max\": 4685.051489257812,\n        \"num_unique_values\": 768,\n        \"samples\": [\n          1944.402384440104,\n          264.51095145089283,\n          1300.5780151367187\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"UNRATE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.694375444803181,\n        \"min\": 3.4,\n        \"max\": 14.8,\n        \"num_unique_values\": 74,\n        \"samples\": [\n          5.5,\n          9.2,\n          4.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FEDFUNDS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.6713459386575096,\n        \"min\": 0.05,\n        \"max\": 19.1,\n        \"num_unique_values\": 469,\n        \"samples\": [\n          4.32,\n          4.51,\n          2.92\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AWHMAN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7341490545598822,\n        \"min\": 37.3,\n        \"max\": 42.3,\n        \"num_unique_values\": 39,\n        \"samples\": [\n          42.0,\n          42.2,\n          39.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"UMCSENT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.699422903806159,\n        \"min\": 50.0,\n        \"max\": 112.0,\n        \"num_unique_values\": 347,\n        \"samples\": [\n          99.8,\n          73.4,\n          79.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TB3SMFFM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.699484409084671,\n        \"min\": -5.37,\n        \"max\": 1.07,\n        \"num_unique_values\": 216,\n        \"samples\": [\n          0.19,\n          0.58,\n          -1.42\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"T10YFFM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.60469835040375,\n        \"min\": -6.51,\n        \"max\": 3.85,\n        \"num_unique_values\": 413,\n        \"samples\": [\n          1.39,\n          0.68,\n          0.21\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CPALTT01USM657N\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.35954869497963793,\n        \"min\": -1.9152895328595805,\n        \"max\": 1.80586907449211,\n        \"num_unique_values\": 679,\n        \"samples\": [\n          0.821890915406006,\n          0.0678426051560472,\n          0.5434782608695671\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"UEMPMEAN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.36528623082303,\n        \"min\": 7.1,\n        \"max\": 40.7,\n        \"num_unique_values\": 232,\n        \"samples\": [\n          7.1,\n          10.4,\n          13.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PERMIT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 380.17540863511306,\n        \"min\": 513.0,\n        \"max\": 2419.0,\n        \"num_unique_values\": 585,\n        \"samples\": [\n          1610.0,\n          1966.0,\n          1370.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"business_cycle\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3294511613091715,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["data=data_original.copy()\n","data=data.drop(['Date'], axis=1)\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"4k-ah8CBrkus","outputId":"aa985723-49e1-4fbc-a7cf-27822247529e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    INDPRO      GDP      SP500  UNRATE  FEDFUNDS  AWHMAN  UMCSENT  TB3SMFFM  \\\n","0  24.1712  542.648  58.029000     5.2      3.99    40.6    100.0      0.36   \n","1  23.9561  542.648  55.775000     4.8      3.97    40.3    100.0     -0.01   \n","2  23.7410  542.648  55.015217     5.4      3.84    40.0     93.3     -0.53   \n","3  23.5528  541.080  55.700000     5.2      3.92    40.0     93.3     -0.69   \n","4  23.5259  541.080  55.215238     5.1      3.85    40.1     93.3     -0.56   \n","\n","   T10YFFM  CPALTT01USM657N  UEMPMEAN  PERMIT  business_cycle  \n","0     0.73        -0.340136      13.5  1092.0             1.0  \n","1     0.52         0.341297      13.1  1088.0             1.0  \n","2     0.41         0.000000      13.0   955.0             1.0  \n","3     0.36         0.340136      12.6  1016.0             1.0  \n","4     0.50         0.000000      11.9  1052.0             0.0  "],"text/html":["\n","  <div id=\"df-4b3bd28b-3983-407d-a763-b803e02d189b\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>INDPRO</th>\n","      <th>GDP</th>\n","      <th>SP500</th>\n","      <th>UNRATE</th>\n","      <th>FEDFUNDS</th>\n","      <th>AWHMAN</th>\n","      <th>UMCSENT</th>\n","      <th>TB3SMFFM</th>\n","      <th>T10YFFM</th>\n","      <th>CPALTT01USM657N</th>\n","      <th>UEMPMEAN</th>\n","      <th>PERMIT</th>\n","      <th>business_cycle</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>24.1712</td>\n","      <td>542.648</td>\n","      <td>58.029000</td>\n","      <td>5.2</td>\n","      <td>3.99</td>\n","      <td>40.6</td>\n","      <td>100.0</td>\n","      <td>0.36</td>\n","      <td>0.73</td>\n","      <td>-0.340136</td>\n","      <td>13.5</td>\n","      <td>1092.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>23.9561</td>\n","      <td>542.648</td>\n","      <td>55.775000</td>\n","      <td>4.8</td>\n","      <td>3.97</td>\n","      <td>40.3</td>\n","      <td>100.0</td>\n","      <td>-0.01</td>\n","      <td>0.52</td>\n","      <td>0.341297</td>\n","      <td>13.1</td>\n","      <td>1088.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>23.7410</td>\n","      <td>542.648</td>\n","      <td>55.015217</td>\n","      <td>5.4</td>\n","      <td>3.84</td>\n","      <td>40.0</td>\n","      <td>93.3</td>\n","      <td>-0.53</td>\n","      <td>0.41</td>\n","      <td>0.000000</td>\n","      <td>13.0</td>\n","      <td>955.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>23.5528</td>\n","      <td>541.080</td>\n","      <td>55.700000</td>\n","      <td>5.2</td>\n","      <td>3.92</td>\n","      <td>40.0</td>\n","      <td>93.3</td>\n","      <td>-0.69</td>\n","      <td>0.36</td>\n","      <td>0.340136</td>\n","      <td>12.6</td>\n","      <td>1016.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>23.5259</td>\n","      <td>541.080</td>\n","      <td>55.215238</td>\n","      <td>5.1</td>\n","      <td>3.85</td>\n","      <td>40.1</td>\n","      <td>93.3</td>\n","      <td>-0.56</td>\n","      <td>0.50</td>\n","      <td>0.000000</td>\n","      <td>11.9</td>\n","      <td>1052.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b3bd28b-3983-407d-a763-b803e02d189b')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-4b3bd28b-3983-407d-a763-b803e02d189b button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-4b3bd28b-3983-407d-a763-b803e02d189b');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-1013d4f1-cc34-4541-a90a-9fc8a269093a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1013d4f1-cc34-4541-a90a-9fc8a269093a')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-1013d4f1-cc34-4541-a90a-9fc8a269093a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data","summary":"{\n  \"name\": \"data\",\n  \"rows\": 768,\n  \"fields\": [\n    {\n      \"column\": \"INDPRO\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 26.31620037668309,\n        \"min\": 22.1009,\n        \"max\": 104.1181,\n        \"num_unique_values\": 764,\n        \"samples\": [\n          62.2108,\n          49.8181,\n          103.0707\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GDP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7374.620364966031,\n        \"min\": 540.197,\n        \"max\": 27956.998,\n        \"num_unique_values\": 256,\n        \"samples\": [\n          19280.084,\n          567.664,\n          2723.883\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SP500\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1079.1392762788457,\n        \"min\": 53.743333544049946,\n        \"max\": 4685.051489257812,\n        \"num_unique_values\": 768,\n        \"samples\": [\n          1944.402384440104,\n          264.51095145089283,\n          1300.5780151367187\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"UNRATE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.694375444803181,\n        \"min\": 3.4,\n        \"max\": 14.8,\n        \"num_unique_values\": 74,\n        \"samples\": [\n          5.5,\n          9.2,\n          4.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FEDFUNDS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.6713459386575096,\n        \"min\": 0.05,\n        \"max\": 19.1,\n        \"num_unique_values\": 469,\n        \"samples\": [\n          4.32,\n          4.51,\n          2.92\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AWHMAN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7341490545598822,\n        \"min\": 37.3,\n        \"max\": 42.3,\n        \"num_unique_values\": 39,\n        \"samples\": [\n          42.0,\n          42.2,\n          39.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"UMCSENT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.699422903806159,\n        \"min\": 50.0,\n        \"max\": 112.0,\n        \"num_unique_values\": 347,\n        \"samples\": [\n          99.8,\n          73.4,\n          79.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TB3SMFFM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.699484409084671,\n        \"min\": -5.37,\n        \"max\": 1.07,\n        \"num_unique_values\": 216,\n        \"samples\": [\n          0.19,\n          0.58,\n          -1.42\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"T10YFFM\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.60469835040375,\n        \"min\": -6.51,\n        \"max\": 3.85,\n        \"num_unique_values\": 413,\n        \"samples\": [\n          1.39,\n          0.68,\n          0.21\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CPALTT01USM657N\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.35954869497963793,\n        \"min\": -1.9152895328595805,\n        \"max\": 1.80586907449211,\n        \"num_unique_values\": 679,\n        \"samples\": [\n          0.821890915406006,\n          0.0678426051560472,\n          0.5434782608695671\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"UEMPMEAN\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.36528623082303,\n        \"min\": 7.1,\n        \"max\": 40.7,\n        \"num_unique_values\": 232,\n        \"samples\": [\n          7.1,\n          10.4,\n          13.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PERMIT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 380.17540863511306,\n        \"min\": 513.0,\n        \"max\": 2419.0,\n        \"num_unique_values\": 585,\n        \"samples\": [\n          1610.0,\n          1966.0,\n          1370.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"business_cycle\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3294511613091715,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":[],"metadata":{"id":"NveHwE4IL4a8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Sequential train/test split**"],"metadata":{"id":"aNZZn7dqzL4a"}},{"cell_type":"code","source":["def sequential_split(data, test_prop=0.2):\n","    # data: either X or y\n","    # Calculate lengths for train and test splits\n","    train_length = int(len(data) * (1 - test_prop))\n","    test_length = len(data) - train_length\n","\n","    # handles pandas dataframe\n","    if isinstance(data, pd.DataFrame):\n","        # Using iloc to handle DataFrame slicing\n","        train_data = data.iloc[:train_length, :]\n","        test_data = data.iloc[train_length:, :]\n","    # handles numpy array\n","    elif isinstance(data, np.ndarray):\n","        # Using standard slicing for 1D NumPy arrays\n","        train_data = data[:train_length]\n","        test_data = data[train_length:len(data)]\n","    else:\n","        raise ValueError(\"The input data must be a pandas DataFrame or a NumPy array.\")\n","\n","    return train_data, test_data"],"metadata":{"id":"MTKvgLqaIgIR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Input creation with given time step for train and test set**\n","\n","This takes the scaled X and y data and outputs input data for for a given time step."],"metadata":{"id":"n40hGtS6zSF9"}},{"cell_type":"code","source":["\n","def DatasetCreation(X_data,y_data, time_step=1): # plug in x_train and y_train/ x_test and y_test\n","  '''\n","  This function creates input data for x_train/y_train or x_test/y_test for\n","   a given time step.\n","   X_data: either X_train or X_test\n","   y_data: either y_train or y_test\n","  '''\n","  # X_array=X_data.values\n","  y_array=np.array(y_data)\n","  X, y =[], []\n","  for i in range(len(X_data)-time_step-1):\n","    x=X_data[i:(i+time_step), ]\n","    X.append(x)\n","    y.append(y_array[i+time_step]) # business cycle: 0 or 1\n","  return np.array(X), np.array(y)"],"metadata":{"id":"jxBLdZkHBE7_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k7p3pTB_MtWj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def write_dic_to_file(dic_name, file_name):\n","  file = open(file_name, 'w')\n","  file.write(str(dic_name))\n","  file.close()"],"metadata":{"id":"oh_n3Qw8Nohk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Fg4gLDdrNqFH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import time\n","from sklearn.preprocessing import StandardScaler"],"metadata":{"id":"d-2TUPlvGH8d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from tensorflow.keras import optimizers"],"metadata":{"id":"3WNiNPgSz2cH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"],"metadata":{"id":"PVg1nf8KOgDb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fTlne6JBOiEP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **LSTM Model**"],"metadata":{"id":"wfga3hPoOkzs"}},{"cell_type":"code","source":["def Build_LSTM_Model(layers, time_step, num_features,\n","                     optimizer='Adam',\n","                     learning_rate=0.001,\n","                     verbose=1):\n","\n","    model = Sequential()\n","\n","    for i in range(len(layers)):\n","        if len(layers) == 1:\n","            model.add(LSTM(int(layers[i]), input_shape=(time_step, num_features)))\n","        else:\n","            if i < len(layers) - 1:\n","                if i == 0:\n","                    model.add(LSTM(int(layers[i]),\n","                                   input_shape=(time_step, num_features),\n","                                   return_sequences=True))\n","                else:\n","                    model.add(LSTM(int(layers[i]), return_sequences=True))\n","            else:\n","                model.add(LSTM(int(layers[i])))\n","\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","    if optimizer == 'Adam':\n","        opt = optimizers.Adam(learning_rate=learning_rate)\n","    elif optimizer == 'Adagrad':\n","        opt = optimizers.Adagrad(learning_rate=learning_rate)\n","    elif optimizer == 'Nadam':\n","        opt = optimizers.Nadam(learning_rate=learning_rate)\n","    elif optimizer == 'Adadelta':\n","        opt = optimizers.Adadelta(learning_rate=learning_rate)\n","    elif optimizer == 'RMSprop':\n","        opt = optimizers.RMSprop(learning_rate=learning_rate)\n","    else:\n","        print(\"No optimizer found among: Adam, Adagrad, Nadam, Adadelta, RMSprop\")\n","\n","    model.compile(loss='binary_crossentropy', optimizer=opt,  metrics = ['accuracy'])\n","\n","    if verbose == 1:\n","        print(model.summary())\n","    return model"],"metadata":{"id":"xNeoU8AYzzyt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizers_names = ['Adam']\n","time_step = 3\n","num_features = 12\n","learning_rate = 0.001\n","verbose = 1\n","layers = [32]\n","\n","Build_LSTM_Model(layers,\n","                 time_step,\n","                 num_features,\n","                 optimizer =  optimizers_names[0],\n","                 learning_rate= learning_rate,\n","                 verbose = verbose)"],"metadata":{"id":"-RCCTT230cRN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["time_step = 3\n","optimizers_names = ['Adam', 'Nadam', 'Adagrad']\n","learning_rates =  [0.01,0.001, 0.005]\n","batch_sizes =  [8, 16,32]\n","epochs = 30\n","num_replicates = 10\n","test_prop = 0.3"],"metadata":{"id":"H2CWrzom8VGD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X=data.drop(['business_cycle'],axis=1)\n","y=np.array(data['business_cycle'])\n","\n","\n","# Splitting train and test data\n","X_train0, X_test0=sequential_split(X, test_prop)\n","y_train0, y_test0=sequential_split(y, test_prop)\n","\n","num_features = X_train0.shape[1]\n","\n","\n","# Normalizing the train and test input using StandardScaler\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train0).reshape(X_train0.shape[0],  num_features)\n","X_test_scaled = scaler.transform(X_test0).reshape(X_test0.shape[0],  num_features)\n","\n","\n","# Creating input data\n","X_train1,  y_train1 =DatasetCreation(X_train_scaled,y_train0, time_step)\n","# 10% of the data equiv with 14% of train goes in validation\n","v=int(len(X_train1)*0.86)\n","X_train=X_train1[0:v]\n","X_val=X_train1[v:]\n","y_train=y_train1[0:v]\n","y_val=y_train1[v:]\n","\n","X_test,  y_test = DatasetCreation(X_test_scaled, y_test0, time_step)"],"metadata":{"id":"PPy_oYPTbdUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train1.shape)\n","print(y_train1.shape)\n","print(y_train.shape)\n","print(y_val.shape)\n","print(y_test.shape)\n","print('Number of 1s in y_train', np.sum(y_train==1))\n","print('Number of 0s in y_train', np.sum(y_train==0))\n","print('Number of 1s in y_val', np.sum(y_val==1))\n","print('Number of 0s in y_val', np.sum(y_val==0))\n","print('Number of 1s in y_test', np.sum(y_test==1))\n","print('Number of 0s in y_test', np.sum(y_test==0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pj463YbI4crl","outputId":"059f50f2-9be1-4dad-cb21-752a8c4796a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(533, 3, 12)\n","(533,)\n","(458,)\n","(75,)\n","(227,)\n","Number of 1s in y_train 391\n","Number of 0s in y_train 67\n","Number of 1s in y_val 67\n","Number of 0s in y_val 8\n","Number of 1s in y_test 207\n","Number of 0s in y_test 20\n"]}]},{"cell_type":"code","source":["def LSTM_Hyper_Parameter_Tuning(layers, data, time_step, test_prop,\n","                                optimizers_names,learning_rates, batch_sizes,\n","                                epochs, num_replicates=2):\n","\n","    X=data.drop(['business_cycle'],axis=1)\n","    y=np.array(data['business_cycle'])\n","\n","\n","    # Splitting train and test data\n","    X_train0, X_test0=sequential_split(X, test_prop)\n","    y_train0, y_test0=sequential_split(y, test_prop)\n","\n","    num_features = X_train0.shape[1]\n","\n","\n","    # Normalizing the train and test input using StandardScaler\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train0).reshape(X_train0.shape[0],  num_features)\n","    X_test_scaled = scaler.transform(X_test0).reshape(X_test0.shape[0],  num_features)\n","\n","\n","    # Creating input data\n","    X_train1,  y_train1 =DatasetCreation(X_train_scaled,y_train0, time_step)\n","    # 10% of the data equiv with 14% of train goes in validation\n","    v=int(len(X_train1)*0.86)\n","    X_train=X_train1[0:v]\n","    X_val=X_train1[v:]\n","    y_train=y_train1[0:v]\n","    y_val=y_train1[v:]\n","\n","    X_test,  y_test = DatasetCreation(X_test_scaled, y_test0, time_step)\n","\n","\n","\n","    # collecting metrices\n","\n","    best_avg_accuracy = 0\n","    collect_accuracy = []\n","\n","    all_avg_accuracy = np.zeros((len(optimizers_names),\n","                                 len(learning_rates),\n","                                 len(batch_sizes)))\n","\n","\n","\n","\n","    best_hyper_parameters = {\"model\": layers,\n","                             \"optimizer\": None,\n","                             \"learning_rate\": None,\n","                             \"batch_size\": None,\n","                             \"best_avg_accuracy\": None}\n","\n","    for opt in range(len(optimizers_names)):\n","\n","        for lr in range(len(learning_rates)):\n","\n","            for batch_size in range(len(batch_sizes)):\n","\n","                for i in range(num_replicates):\n","\n","                    print(\"Running for \" + optimizers_names[opt] + \" optimizer \"\\\n","                           + str(learning_rates[lr]) + \" learning_rate \"\\\n","                          + str(batch_sizes[batch_size]) +\" batch_size and \"\\\n","                           + str(i) +\" replicate \" + \"\\n\")\n","\n","\n","                    model = Build_LSTM_Model(layers,\n","                                             time_step,\n","                                             num_features,\n","                                             optimizers_names[opt],\n","                                             learning_rate=learning_rates[lr],\n","                                             verbose=0)\n","\n","                    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n","                                                                patience=5)\n","\n","                    history = model.fit(X_train, y_train,\n","                                        batch_size=batch_sizes[batch_size],\n","                                        epochs=epochs,\n","                                        validation_data=(X_val, y_val),\n","                                        callbacks=[callback], verbose=1)\n","\n","\n","                    # Predict probabilities for each class\n","                    test_pred_probs = model.predict(X_test)\n","\n","                    # Threshold the probabilities to get class labels\n","                    test_pred = (test_pred_probs > 0.5).astype(int)\n","\n","                    # Compute evaluation metrics\n","                    accuracy = accuracy_score(y_test, test_pred)\n","\n","\n","                    collect_accuracy.append(accuracy)\n","\n","\n","                avg_accuracy = np.mean(np.array(collect_accuracy))\n","                all_avg_accuracy[opt][lr][batch_size] = avg_accuracy\n","\n","\n","\n","                if avg_accuracy > best_avg_accuracy:\n","                  best_avg_accuracy = avg_accuracy\n","                  best_hyper_parameters = {\"model\": layers,\n","                                             \"optimizer\": optimizers_names[opt],\n","                                             \"learning_rate\": learning_rates[lr],\n","                                             \"batch_size\": batch_sizes[batch_size],\n","                                             \"best_avg_accuracy\": best_avg_accuracy\n","                                         }\n","\n","    output_dictionary = {\n","        \"best_hyper_parameters\": best_hyper_parameters,\n","        \"all_avg_accuracy\": all_avg_accuracy\n","    }\n","\n","    # writing output dictionary in the file\n","\n","    file_name = output_path + \"lstm-\" + str(layers[0]) + \"N-hyperparameter_tuning__results\" + \".txt\"\n","    write_dic_to_file(output_dictionary, file_name)\n","\n","    print(\"Best_hyper_parameters(LSTM): \\n\", output_dictionary['best_hyper_parameters'])\n","    print(\"all_avg_accuracy(LSTM): \\n\", output_dictionary['all_avg_accuracy'])\n","\n","    return output_dictionary['best_hyper_parameters']"],"metadata":{"id":"PBsoLFR_5WSh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Bz7bPLpiDh26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ozoV3o9D51Mg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layers = [4]\n","\n","lstm_N4_best_hyper_parameters = LSTM_Hyper_Parameter_Tuning(layers, data,\n","                                                             time_step,\n","                                                             test_prop,\n","                                                             optimizers_names,\n","                                                             learning_rates,\n","                                                             batch_sizes,\n","                                                             epochs = epochs,\n","                                                             num_replicates=num_replicates)\n","lstm_N4_best_hyper_parameters"],"metadata":{"id":"Xv3dHnJkHZyY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# {'model': [4],\n","#  'optimizer': 'Adam',\n","#  'learning_rate': 0.005,\n","#  'batch_size': 8,\n","#  'best_avg_accuracy': 0.8310887350534927}"],"metadata":{"id":"yH0HV1uWpLJu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layers = [8]\n","\n","lstm_N8_best_hyper_parameters = LSTM_Hyper_Parameter_Tuning(layers, data,\n","                                                             time_step,\n","                                                             test_prop,\n","                                                             optimizers_names,\n","                                                             learning_rates,\n","                                                             batch_sizes,\n","                                                             epochs = epochs,\n","                                                             num_replicates=num_replicates)\n","lstm_N8_best_hyper_parameters"],"metadata":{"id":"B3yQTcljKp9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# {'model': [8],\n","#  'optimizer': 'Nadam',\n","#  'learning_rate': 0.005,\n","#  'batch_size': 16,\n","#  'best_avg_accuracy': 0.8701995335579165}"],"metadata":{"id":"OqRGnlJI1HP1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layers = [16]\n","\n","lstm_N16_best_hyper_parameters = LSTM_Hyper_Parameter_Tuning(layers, data,\n","                                                             time_step,\n","                                                             test_prop,\n","                                                             optimizers_names,\n","                                                             learning_rates,\n","                                                             batch_sizes,\n","                                                             epochs = epochs,\n","                                                             num_replicates=num_replicates)\n","lstm_N16_best_hyper_parameters"],"metadata":{"id":"fklVNst7h1Dh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# {'model': [16],\n","#  'optimizer': 'Adam',\n","#  'learning_rate': 0.001,\n","#  'batch_size': 32,\n","#  'best_avg_accuracy': 0.8914831130690161}"],"metadata":{"id":"Ha-f00OpHVud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layers = [32]\n","\n","lstm_N32_best_hyper_parameters = LSTM_Hyper_Parameter_Tuning(layers, data,\n","                                                             time_step,\n","                                                             test_prop,\n","                                                             optimizers_names,\n","                                                             learning_rates,\n","                                                             batch_sizes,\n","                                                             epochs = epochs,\n","                                                             num_replicates=num_replicates)\n","lstm_N32_best_hyper_parameters"],"metadata":{"id":"wD2_v9Hmicn5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# {'model': [32],\n","#  'optimizer': 'Nadam',\n","#  'learning_rate': 0.005,\n","#  'batch_size': 32,\n","#  'best_avg_accuracy': 0.9012481644640237}"],"metadata":{"id":"FhaUMxYoJyaX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layers = [64]\n","\n","lstm_N64_best_hyper_parameters = LSTM_Hyper_Parameter_Tuning(layers, data,\n","                                                             time_step,\n","                                                             test_prop,\n","                                                             optimizers_names,\n","                                                             learning_rates,\n","                                                             batch_sizes,\n","                                                             epochs = epochs,\n","                                                             num_replicates=num_replicates)\n","lstm_N64_best_hyper_parameters"],"metadata":{"id":"x8jowZnqitar"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# {'model': [64],\n","#  'optimizer': 'Nadam',\n","#  'learning_rate': 0.005,\n","#  'batch_size': 32,\n","#  'best_avg_accuracy': 0.908516886930984}"],"metadata":{"id":"3PC8qqPicYtH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layers = [128]\n","\n","lstm_N128_best_hyper_parameters = LSTM_Hyper_Parameter_Tuning(layers, data,\n","                                                             time_step,\n","                                                             test_prop,\n","                                                             optimizers_names,\n","                                                             learning_rates,\n","                                                             batch_sizes,\n","                                                             epochs = epochs,\n","                                                             num_replicates=num_replicates)\n","lstm_N128_best_hyper_parameters"],"metadata":{"id":"IEeBAyk7hmDw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# {'model': [128],\n","#  'optimizer': 'Nadam',\n","#  'learning_rate': 0.005,\n","#  'batch_size': 32,\n","#  'best_avg_accuracy': 0.9079295154185025}"],"metadata":{"id":"mcxZ-gwOnX-y"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RbAtH3KenO9D"},"outputs":[],"source":["def LSTM_Model(layers,\n","               hyper_parameters,\n","               data,\n","               time_step = 1,\n","               test_prop = 0.2,\n","               epochs = 5,\n","               num_replicates = 2):\n","\n","    X=data.drop(['business_cycle'],axis=1)\n","    y=np.array(data['business_cycle'])\n","\n","\n","    # Splitting train and test data\n","    X_train0, X_test0=sequential_split(X, test_prop)\n","    y_train0, y_test0=sequential_split(y, test_prop)\n","\n","    num_features = X_train0.shape[1]\n","\n","\n","    # Normalizing the train and test input using StandardScaler\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train0).reshape(X_train0.shape[0],  num_features)\n","    X_test_scaled = scaler.transform(X_test0).reshape(X_test0.shape[0],  num_features)\n","\n","\n","    # Creating input data\n","    X_train,  y_train =DatasetCreation(X_train_scaled,y_train0, time_step)\n","    X_test,  y_test = DatasetCreation(X_test_scaled, y_test0, time_step)\n","\n","\n","    # arrays for collecting test scores\n","    accuracy_array = np.zeros(num_replicates)\n","    precision_array = np.zeros(num_replicates)\n","    recall_array    = np.zeros(num_replicates)\n","    f1score_array    = np.zeros(num_replicates)\n","    sensitivity_array    = np.zeros(num_replicates)\n","    specificity_array    = np.zeros(num_replicates)\n","    elapsed_time_array = np.zeros(num_replicates)\n","\n","\n","    models_history = []\n","    train_predictions = []\n","    test_predictions = []\n","\n","    for i in range(num_replicates):\n","\n","      print(\"Program is running for %d replicate ----->\\n\" %i)\n","\n","      model = Build_LSTM_Model(layers, time_step, num_features,\n","                               optimizer = hyper_parameters[0],\n","                               learning_rate = hyper_parameters[1], verbose = 0)\n","\n","      callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience= 5)\n","      # This callback will stop the training when there is no improvement in\n","      # the loss for three consecutive epochs\n","      start = time.time()\n","\n","      history = model.fit(X_train, y_train, batch_size = hyper_parameters[2],\n","                          epochs= epochs, callbacks=[callback], verbose = 1)\n","\n","      end = time.time()\n","      elapsed_time = end - start\n","\n","      models_history.append(history)\n","\n","\n","      # Train and test prediction\n","      # Predict probabilities for each class\n","\n","      train_pred_probs = model.predict(X_train)\n","      # Threshold the probabilities to get class labels\n","      train_pred = (train_pred_probs > 0.5).astype(int)\n","\n","\n","      test_pred_probs = model.predict(X_test)\n","      # Threshold the probabilities to get class labels\n","      test_pred = (test_pred_probs > 0.5).astype(int)\n","\n","      # Compute evaluation metrics\n","      accuracy = accuracy_score(y_test, test_pred)\n","      precision = precision_score(y_test, test_pred)\n","      recall = recall_score(y_test, test_pred)\n","      f1 = f1_score(y_test, test_pred)\n","      tn, fp, fn, tp = confusion_matrix(y_test, test_pred).ravel()\n","      sensitivity = tp / (tp + fn)\n","      specificity = tn / (tn + fp)\n","\n","      train_predictions.append(train_pred)\n","      test_predictions.append(test_pred)\n","\n","\n","      accuracy_array[i] =  accuracy\n","      precision_array[i] = precision\n","      recall_array[i] = recall\n","      f1score_array[i] = f1\n","      sensitivity_array[i] = sensitivity\n","      specificity_array[i]=specificity\n","      elapsed_time_array[i] = elapsed_time\n","\n","    max_index = accuracy_array.argmax()\n","    best_accuracy = accuracy_array[max_index]\n","    precision_with_best_accuracy = precision_array[max_index]\n","    recall_with_best_accuracy =  recall_array[max_index]\n","    f1score_with_best_accuracy=f1score_array[max_index]\n","    sensitivity_with_best_accuracy=sensitivity_array[max_index]\n","    specificity_with_best_accuracy=specificity_array[max_index]\n","    elapsed_time_with_best_accuracy = elapsed_time_array[max_index]\n","\n","    train_predictions_with_best_accuracy = train_predictions[max_index]\n","    test_predictions_with_best_accuracy = test_predictions[max_index]\n","\n","    loss_with_best_accuracy = models_history[max_index].history['loss']\n","\n","    #val_loss_with_best_accuracy = models_history[max_index].history['val_loss']\n","\n","\n","    # Collecting important results\n","    performance_metrics =  {\n","\n","                        'scores': {'accuracy': accuracy_array,\n","                                    'precision': precision_array,\n","                                    'recall': recall_array,\n","                                    'f1':f1score_array,\n","                                   'sensitivity':sensitivity_array,\n","                                   'specificity':specificity_array,\n","                                    'elapsed_time': elapsed_time_array\n","                                    },\n","\n","                        'minimums': {'accuracy': np.min(accuracy_array),\n","                                      'precision': np.min(precision_array),\n","                                      'recall': np.min(recall_array),\n","                                      'f1': np.min(f1score_array),\n","                                      'sensitivity':np.min(sensitivity_array),\n","                                      'specificity': np.min(specificity_array),\n","                                      'elapsed_time': np.min(elapsed_time_array)\n","                                      },\n","\n","                        'avg_scores':  {'accuracy': np.mean(accuracy_array),\n","                                      'precision': np.mean(precision_array),\n","                                      'recall': np.mean(recall_array),\n","                                      'f1': np.mean(f1score_array),\n","                                      'sensitivity':np.mean(sensitivity_array),\n","                                      'specificity': np.mean(specificity_array),\n","                                      'elapsed_time': np.mean(elapsed_time_array)\n","                                      },\n","\n","                          'stds':     {'accuracy': np.std(accuracy_array),\n","                                      'precision': np.std(precision_array),\n","                                      'recall': np.std(recall_array),\n","                                      'f1': np.std(f1score_array),\n","                                      'sensitivity':np.std(sensitivity_array),\n","                                      'specificity': np.std(specificity_array),\n","                                      'elapsed_time': np.std(elapsed_time_array)\n","                                      },\n","\n","                        'maximums': {'accuracy': np.max(accuracy_array),\n","                                      'precision': np.max(precision_array),\n","                                      'recall': np.max(recall_array),\n","                                      'f1': np.max(f1score_array),\n","                                      'sensitivity':np.max(sensitivity_array),\n","                                      'specificity': np.max(specificity_array),\n","                                      'elapsed_time': np.max(elapsed_time_array)\n","                                      }\n","\n","                  }\n","\n","\n","\n","\n","    model_with_best_accuracy = {\n","\n","                            'replicate': max_index,\n","                            'accuracy': best_accuracy,\n","                            'precision': precision_with_best_accuracy,\n","                            'recall': recall_with_best_accuracy,\n","                            'f1': f1score_with_best_accuracy,\n","                            'sensitivity': sensitivity_with_best_accuracy,\n","                            'specificity': specificity_with_best_accuracy,\n","                            'elapsed_time': elapsed_time_with_best_accuracy,\n","                            'train_predictions':train_predictions_with_best_accuracy,\n","                            'test_predictions': test_predictions_with_best_accuracy,\n","                            'loss':loss_with_best_accuracy\n","                             #'val_loss': val_loss_with_best_accuracy\n","                            }\n","\n","     # Collecting hyperparameters\n","    hyper_parameters = {'layers': layers,\n","                        'model_specific_hyper_parameters': hyper_parameters,\n","                       'epochs': epochs,\n","                       'time_step':time_step,\n","                       'num_replicates': num_replicates,\n","                       'test_prop':test_prop\n","                        }\n","\n","\n","     #======= Collecting all the outputs together =============#\n","    output_dictionary = {'hyper_parameters': hyper_parameters,\n","                        'performance_metrics': performance_metrics,\n","                         'best_model': model_with_best_accuracy,\n","                       }\n","\n","    print(\"Progress: All works are done successfully, congratulations!!\\n\")\n","    return output_dictionary\n"]},{"cell_type":"code","source":["hyper_parameters= ['Adam', 0.001, 32]\n","layers=[8]\n","lstm_output= LSTM_Model(layers,\n","               hyper_parameters,\n","                        data,\n","               time_step = 1,\n","               test_prop = 0.3,\n","               epochs = 5,\n","               num_replicates = 2)\n","lstm_output"],"metadata":{"id":"EhZFoRbD7KeU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-pqni7frZ02d"},"source":["## **Executing Multiple LSTM Models**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GugZupYxeSc"},"outputs":[],"source":["def Multiple_LSTM_Models(hidden_layers,\n","                         hyper_parameters,\n","                         data,\n","                         time_step = 1,\n","                         test_prop = 0.2,\n","                         epochs = 5,\n","                         num_replicates = 2):\n","\n","  num_models = len(hidden_layers)\n","\n","  # collect all scores\n","  accuracy = []\n","  precision = []\n","  recall = []\n","  f1 = []\n","  sensitivity = []\n","  specificity = []\n","  elapsed_time = []\n","\n","  # collect all avg scores\n","  avg_accuracy = []\n","  avg_precision = []\n","  avg_recall = []\n","  avg_f1 = []\n","  avg_sensitivity = []\n","  avg_specificity = []\n","  avg_elapsed_time = []\n","\n","  # iteratively update the best rmse and the corresponding model\n","  best_accuracy = 0\n","  best_avg_accuracy = 0\n","  best_model_hidden_layers = None\n","  best_model_output = None\n","\n","  for i in range(num_models):\n","    #print(\"Running model with hidden neurons: \", hidden_layers[i])\n","\n","    #print(\"\\n\")\n","\n","    #print(\"Best Hyper_parameters used: \", hyper_parameters[i])\n","\n","    #print(\"\\n\")\n","\n","    output = LSTM_Model(hidden_layers[i],\n","                        hyper_parameters[i],\n","                        data,\n","                        time_step,\n","                        test_prop,\n","                        epochs,\n","                        num_replicates)\n","\n","\n","    accuracy.append(output['performance_metrics']['scores']['accuracy'])\n","    precision.append(output['performance_metrics']['scores']['precision'])\n","    recall.append(output['performance_metrics']['scores']['recall'])\n","    f1.append(output['performance_metrics']['scores']['f1'])\n","    sensitivity.append(output['performance_metrics']['scores']['sensitivity'])\n","    specificity.append(output['performance_metrics']['scores']['specificity'])\n","    elapsed_time.append(output['performance_metrics']['scores']['elapsed_time'])\n","\n","    avg_accuracy.append(output['performance_metrics']['avg_scores']['accuracy'])\n","    avg_precision.append(output['performance_metrics']['avg_scores']['precision'])\n","    avg_recall.append(output['performance_metrics']['avg_scores']['recall'])\n","    avg_f1.append(output['performance_metrics']['avg_scores']['f1'])\n","    avg_sensitivity.append(output['performance_metrics']['avg_scores']['sensitivity'])\n","    avg_specificity.append(output['performance_metrics']['avg_scores']['specificity'])\n","    avg_elapsed_time.append(output['performance_metrics']['avg_scores']['elapsed_time'])\n","\n","    if avg_accuracy[i] > best_avg_accuracy:\n","      best_avg_accuracy = avg_accuracy[i]\n","      best_accuracy = output['best_model']['accuracy']\n","      best_model_hidden_layers = hidden_layers[i]\n","      best_model_output = output\n","\n","\n","  accuracy = np.array(accuracy)\n","  precision = np.array(precision)\n","  recall =  np.array(recall)\n","  f1 = np.array(f1)\n","  sensitivity = np.array(sensitivity)\n","  specificity =  np.array(specificity)\n","\n","  # Collecting all scores\n","\n","  performance_metrics = {\n","\n","       'scores':  {'layers': hidden_layers,\n","                   'accuracy': accuracy,\n","                   'precision': precision,\n","                   'recall':recall,\n","                   'f1': f1,\n","                   'sensitivity': sensitivity,\n","                   'specificity':specificity,\n","                   'elapsed_time': elapsed_time },\n","\n","       'avg_scores':  pd.DataFrame({'layers': hidden_layers,\n","                                    'accuracy': np.array(avg_accuracy),\n","                                    'precision': np.array(avg_precision),\n","                                    'recall':np.array(avg_recall),\n","                                    'f1': np.array(avg_f1),\n","                                    'sensitivity': np.array(avg_sensitivity),\n","                                    'specificity':np.array(avg_specificity),\n","                                    'elapsed_time':np.array(avg_elapsed_time)}),\n","\n","       'stds':     pd.DataFrame({'layers': hidden_layers,\n","                                 'accuracy': np.std(accuracy, axis = 1),\n","                                 'precision': np.std(precision, axis = 1),\n","                                 'recall':  np.std(recall, axis = 1 ),\n","                                 'f1': np.std(f1, axis = 1),\n","                                 'sensitivity': np.std(sensitivity, axis = 1),\n","                                 'specificity':  np.std(specificity, axis = 1 ),\n","                                 'elapsed_time': np.std(elapsed_time, axis = 1 )}),\n","\n","       'minimums': pd.DataFrame({'layers': hidden_layers,\n","                                'accuracy': np.min(accuracy, axis =1 ),\n","                                'precision': np.min(precision, axis= 1),\n","                                'recall': np.min(recall, axis =1),\n","                                'f1': np.min(f1, axis =1 ),\n","                                'sensitivity': np.min(sensitivity, axis= 1),\n","                                'specificity': np.min(specificity, axis =1),\n","                                'elapsed_time': np.min(elapsed_time, axis =1)}),\n","\n","       'maximums': pd.DataFrame({'layers': hidden_layers,\n","                                'accuracy': np.max(accuracy, axis =1),\n","                                'precision': np.max(precision, axis =1),\n","                                'recall': np.max(recall, axis =1),\n","                                'f1': np.max(f1, axis =1),\n","                                'sensitivity': np.max(sensitivity, axis =1),\n","                                'specificity': np.max(specificity, axis =1),\n","                                'elapsed_time': np.max(elapsed_time,axis =1)})\n","    }\n","\n","\n","  output_dictionary = {\n","                     'hyper_parameters': hyper_parameters,\n","\n","                      'best_avg_accuracy': best_avg_accuracy,\n","                      'best_accuracy': best_accuracy,\n","                      'best_model_hidden_layers': best_model_hidden_layers,\n","                      'best_model_output': best_model_output\n","                      }\n","\n","  #Save all statistics:\n","  performance_metrics['avg_scores'].to_csv(output_path+'multiple_lstm_models_average_scores.csv')\n","  performance_metrics['stds'].to_csv(output_path+'multiple_lstm_models_stds.csv')\n","  performance_metrics['minimums'].to_csv(output_path+'multiple_lstm_models_minimums.csv')\n","  performance_metrics['maximums'].to_csv(output_path+'multiple_lstm_models_maximums.csv')\n","\n","\n","  #Save all scores in the file for future analysis\n","  pd.DataFrame(performance_metrics['scores']['accuracy']).to_csv(output_path+'multiple_lstm_models_all_accuracy.csv')\n","  pd.DataFrame(performance_metrics['scores']['precision']).to_csv(output_path+'multiple_lstm_models_all_precision.csv')\n","  pd.DataFrame(performance_metrics['scores']['recall']).to_csv(output_path+'multiple_lstm_models_all_recall.csv')\n","  pd.DataFrame(performance_metrics['scores']['f1']).to_csv(output_path+'multiple_lstm_models_all_f1.csv')\n","  pd.DataFrame(performance_metrics['scores']['sensitivity']).to_csv(output_path+'multiple_lstm_models_all_sensitivity.csv')\n","  pd.DataFrame(performance_metrics['scores']['specificity']).to_csv(output_path+'multiple_lstm_models_all_specificity.csv')\n","\n","  #Save best model results\n","  pd.DataFrame(best_model_output['best_model']['loss']).to_csv(output_path+'best_lstm_model_loss.csv')\n","  pd.DataFrame(best_model_output['best_model']['train_predictions']).to_csv(output_path+'best_lstm_model_train_predictions.csv')\n","  pd.DataFrame(best_model_output['best_model']['test_predictions']).to_csv(output_path+'best_lstm_model_test_predictions.csv')\n","  pd.DataFrame(best_model_output['performance_metrics']['scores']['accuracy']).to_csv(output_path+'best_lstm_model_all_accuracy.csv')\n","\n","  #writing all result in the file\n","  write_dic_to_file(output_dictionary,\n","                    output_path + \"multiple_lstm_models_full_results.txt\")\n","\n","  #Display some key results in the screen\n","  print(\"\\nBest model and its avg accuracy and maximum accuracy):\\n\", best_model_hidden_layers, best_avg_accuracy, best_accuracy)\n","  print(\"Hyper_parameters:\\n\", hyper_parameters)\n","  print('\\nAverage scores:\\n',  performance_metrics['avg_scores'])\n","  print('\\nStandard_deviations:\\n',  performance_metrics['stds'])\n","  print('\\nMinimums:\\n',  performance_metrics['minimums'])\n","  print('\\nMaximums:\\n',  performance_metrics['maximums'])\n","  print(\"Progress: All works are done successfully, congratulations!!\\n\")"]},{"cell_type":"markdown","metadata":{"id":"j5mYB8eUPsRf"},"source":["### **Execution**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hrnEdcrSPuE_"},"outputs":[],"source":["hidden_layers = [[4], [8], [16], [32],[64],[128]]\n","best_hyper_parameters   = [\n","                            ['Adam', 0.005, 8], #4N model\n","                            ['Nadam', 0.005, 16],  #8N model\n","                            ['Adam', 0.001, 32], #16N model\n","                            ['Nadam', 0.005, 32], #32N model\n","                            ['Nadam', 0.005, 32], #64 N model\n","                            ['Nadam', 0.005, 32] #128N model\n","                        ]\n","\n","Multiple_LSTM_Models(hidden_layers,\n","                     best_hyper_parameters,\n","                     data,\n","                     time_step = 3,\n","                     test_prop = 0.3,\n","                     epochs = 30,\n","                     num_replicates = 30)"]},{"cell_type":"code","source":["# Average scores:\n","#    layers  accuracy  precision    recall        f1  sensitivity  specificity  \\\n","# 0    [4]  0.595448   0.937682  0.595813  0.707133     0.595813     0.591667\n","# 1    [8]  0.688253   0.930411  0.710467  0.800391     0.710467     0.458333\n","# 2   [16]  0.828047   0.936098  0.870853  0.900946     0.870853     0.385000\n","# 3   [32]  0.729662   0.926630  0.763607  0.835916     0.763607     0.378333\n","# 4   [64]  0.739648   0.926998  0.775362  0.843826     0.775362     0.370000\n","# 5  [128]  0.726285   0.925235  0.761192  0.834416     0.761192     0.365000\n","\n","#    elapsed_time\n","# 0     14.633699\n","# 1     11.351831\n","# 2      7.316849\n","# 3      8.135706\n","# 4      8.827169\n","# 5     11.728423\n","\n","# Standard_deviations:\n","#    layers  accuracy  precision    recall        f1  sensitivity  specificity  \\\n","# 0    [4]  0.173160   0.023077  0.198704  0.172141     0.198704     0.169353\n","# 1    [8]  0.104575   0.008789  0.118313  0.082138     0.118313     0.065933\n","# 2   [16]  0.058481   0.009001  0.066985  0.038267     0.066985     0.096738\n","# 3   [32]  0.059403   0.006538  0.065291  0.042495     0.065291     0.030777\n","# 4   [64]  0.040763   0.005133  0.044534  0.028579     0.044534     0.027689\n","# 5  [128]  0.044078   0.003996  0.049811  0.031729     0.049811     0.029297\n","\n","#    elapsed_time\n","# 0      3.337256\n","# 1      1.747687\n","# 2      1.047211\n","# 3      0.872982\n","# 4      2.390827\n","# 5      2.323720\n","\n","# Minimums:\n","#    layers  accuracy  precision    recall        f1  sensitivity  specificity  \\\n","# 0    [4]  0.193833   0.896226  0.125604  0.221277     0.125604         0.30\n","# 1    [8]  0.462555   0.912000  0.449275  0.603896     0.449275         0.35\n","# 2   [16]  0.651982   0.902913  0.685990  0.782369     0.685990         0.00\n","# 3   [32]  0.585903   0.912409  0.603865  0.726744     0.603865         0.30\n","# 4   [64]  0.634361   0.913333  0.661836  0.767507     0.661836         0.30\n","# 5  [128]  0.603524   0.914894  0.623188  0.741379     0.623188         0.30\n","\n","#    elapsed_time\n","# 0     11.887335\n","# 1      8.421127\n","# 2      5.773108\n","# 3      6.377988\n","# 4      6.493367\n","# 5      7.768668\n","\n","# Maximums:\n","#    layers  accuracy  precision    recall        f1  sensitivity  specificity  \\\n","# 0    [4]  0.889868   1.000000  0.932367  0.939173     0.932367         1.00\n","# 1    [8]  0.867841   0.949239  0.903382  0.925743     0.903382         0.60\n","# 2   [16]  0.938326   0.948571  0.995169  0.967136     0.995169         0.55\n","# 3   [32]  0.828194   0.942105  0.874396  0.902743     0.874396         0.45\n","# 4   [64]  0.797357   0.934426  0.840580  0.883249     0.840580         0.40\n","# 5  [128]  0.797357   0.933702  0.845411  0.883838     0.845411         0.40\n","\n","#    elapsed_time\n","# 0     23.468831\n","# 1     14.484608\n","# 2     10.707877\n","# 3     10.502277\n","# 4     18.378201\n","# 5     16.303485\n","# Progress: All works are done successfully, congratulations!!"],"metadata":{"id":"yygRqsSd6oJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"c_gweD4J6oED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"deRbYI7C6n-k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **GRU Model**"],"metadata":{"id":"oh6ZdOD7hjnl"}},{"cell_type":"code","source":["from tensorflow.keras.layers import GRU"],"metadata":{"id":"sK4YeJdwi0-D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Build_GRU_Model(layers, time_step, num_features,\n","                     optimizer='Adam',\n","                     learning_rate=0.001, verbose=1):\n","\n","    model = Sequential()\n","\n","    for i in range(len(layers)):\n","        if len(layers) == 1:\n","            model.add(GRU(int(layers[i]), input_shape=(time_step, num_features)))\n","        else:\n","            if i < len(layers) - 1:\n","                if i == 0:\n","                    model.add(GRU(int(layers[i]),\n","                                   input_shape=(time_step, num_features),\n","                                   return_sequences=True))\n","                else:\n","                    model.add(GRU(int(layers[i]), return_sequences=True))\n","            else:\n","                model.add(GRU(int(layers[i])))\n","\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","    if optimizer == 'Adam':\n","        opt = optimizers.Adam(learning_rate=learning_rate)\n","    elif optimizer == 'Adagrad':\n","        opt = optimizers.Adagrad(learning_rate=learning_rate)\n","    elif optimizer == 'Nadam':\n","        opt = optimizers.Nadam(learning_rate=learning_rate)\n","    elif optimizer == 'Adadelta':\n","        opt = optimizers.Adadelta(learning_rate=learning_rate)\n","    elif optimizer == 'RMSprop':\n","        opt = optimizers.RMSprop(learning_rate=learning_rate)\n","    else:\n","        print(\"No optimizer found among [Adam, Adagrad, Nadam, Adadelta,RMSprop]\")\n","\n","    model.compile(loss='binary_crossentropy', optimizer=opt, metrics = ['accuracy'])\n","\n","    if verbose == 1:\n","        print(model.summary())\n","    return model"],"metadata":{"id":"N1z9q9GLhjXU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizers_names = ['Adam']\n","time_step = 1\n","num_features = 12\n","learning_rate = 0.001\n","verbose = 1\n","layers = [50,20]\n","\n","Build_GRU_Model(layers,\n","                 time_step,\n","                 num_features,\n","                 optimizer =  optimizers_names[0],\n","                 learning_rate= learning_rate,\n","                 verbose = verbose)"],"metadata":{"id":"2GtS2Qo-hjTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DCg3crz2hjRn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"deMZzUV5hjPe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IOqvGkm3IzTk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def GRU_Hyper_Parameter_Tuning(layers, data, time_step, test_prop,\n","                                optimizers_names,learning_rates, batch_sizes,\n","                                epochs, num_replicates=2):\n","\n","    X=data.drop(['business_cycle'],axis=1)\n","    y=np.array(data['business_cycle'])\n","\n","\n","    # Splitting train and test data\n","    X_train0, X_test0=sequential_split(X, test_prop)\n","    y_train0, y_test0=sequential_split(y, test_prop)\n","\n","    num_features = X_train0.shape[1]\n","\n","\n","    # Normalizing the train and test input using StandardScaler\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train0).reshape(X_train0.shape[0],  num_features)\n","    X_test_scaled = scaler.transform(X_test0).reshape(X_test0.shape[0],  num_features)\n","\n","\n","    # Creating input data\n","    X_train1,  y_train1 =DatasetCreation(X_train_scaled,y_train0, time_step)\n","    # 10% of the data equiv with 14% of train goes in validation\n","    v=int(len(X_train1)*0.86)\n","    X_train=X_train1[0:v]\n","    X_val=X_train1[v:]\n","    y_train=y_train1[0:v]\n","    y_val=y_train1[v:]\n","\n","    X_test,  y_test = DatasetCreation(X_test_scaled, y_test0, time_step)\n","\n","\n","\n","    # collecting metrices\n","\n","    best_avg_accuracy = 0\n","    collect_accuracy = []\n","\n","    all_avg_accuracy = np.zeros((len(optimizers_names),\n","                                 len(learning_rates),\n","                                 len(batch_sizes)))\n","\n","\n","\n","\n","    best_hyper_parameters = {\"model\": layers,\n","                             \"optimizer\": None,\n","                             \"learning_rate\": None,\n","                             \"batch_size\": None,\n","                             \"best_avg_accuracy\": None}\n","\n","    for opt in range(len(optimizers_names)):\n","\n","        for lr in range(len(learning_rates)):\n","\n","            for batch_size in range(len(batch_sizes)):\n","\n","                for i in range(num_replicates):\n","\n","                    print(\"Running for \" + optimizers_names[opt] + \" optimizer \"\\\n","                           + str(learning_rates[lr]) + \" learning_rate \"\\\n","                          + str(batch_sizes[batch_size]) +\" batch_size and \"\\\n","                           + str(i) +\" replicate \" + \"\\n\")\n","\n","\n","                    model = Build_GRU_Model(layers,\n","                                             time_step,\n","                                             num_features,\n","                                             optimizers_names[opt],\n","                                             learning_rate=learning_rates[lr],\n","                                             verbose=0)\n","\n","                    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n","                                                                patience=5)\n","\n","                    history = model.fit(X_train, y_train,\n","                                        batch_size=batch_sizes[batch_size],\n","                                        epochs=epochs,\n","                                        validation_data=(X_test, y_test),\n","                                        callbacks=[callback], verbose=1)\n","\n","\n","                    # Predict probabilities for each class\n","                    test_pred_probs = model.predict(X_test)\n","\n","                    # Threshold the probabilities to get class labels\n","                    test_pred = (test_pred_probs > 0.5).astype(int)\n","\n","                    # Compute evaluation metrics\n","                    accuracy = accuracy_score(y_test, test_pred)\n","\n","\n","                    collect_accuracy.append(accuracy)\n","\n","\n","                avg_accuracy = np.mean(np.array(collect_accuracy))\n","                all_avg_accuracy[opt][lr][batch_size] = avg_accuracy\n","\n","\n","\n","                if avg_accuracy > best_avg_accuracy:\n","                  best_avg_accuracy = avg_accuracy\n","                  best_hyper_parameters = {\"model\": layers,\n","                                             \"optimizer\": optimizers_names[opt],\n","                                             \"learning_rate\": learning_rates[lr],\n","                                             \"batch_size\": batch_sizes[batch_size],\n","                                             \"best_avg_accuracy\": best_avg_accuracy\n","                                         }\n","\n","    output_dictionary = {\n","        \"best_hyper_parameters\": best_hyper_parameters,\n","        \"all_avg_accuracy\": all_avg_accuracy\n","    }\n","\n","    # writing output dictionary in the file\n","\n","    file_name = output_path + \"gru-\" + str(layers[0]) + \"N-hyperparameter_tuning__results\" + \".txt\"\n","    write_dic_to_file(output_dictionary, file_name)\n","\n","    print(\"Best_hyper_parameters(GRU): \\n\", output_dictionary['best_hyper_parameters'])\n","    print(\"all_avg_accuracy(GRU): \\n\", output_dictionary['all_avg_accuracy'])\n","\n","    return output_dictionary['best_hyper_parameters']"],"metadata":{"id":"Y9QIhyH8UUGf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8wiAdjzMJCyD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["time_step = 3\n","optimizers_names = ['Adam', 'Nadam', 'Adagrad']\n","learning_rates =  [0.01,0.005, 0.001]\n","batch_sizes =  [8, 16,32]\n","epochs = 30\n","num_replicates = 10\n","test_prop = 0.3"],"metadata":{"id":"DQbRKUc2JDdR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layers = [4]\n","\n","gru_N4_best_hyper_parameters = GRU_Hyper_Parameter_Tuning(layers, data,\n","                                                             time_step,\n","                                                             test_prop,\n","                                                             optimizers_names,\n","                                                             learning_rates,\n","                                                             batch_sizes,\n","                                                             epochs = epochs,\n","                                                             num_replicates=num_replicates)\n","gru_N4_best_hyper_parameters"],"metadata":{"id":"P-TLCzEgJDdS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# {'model': [4],\n","#  'optimizer': 'Adagrad',\n","#  'learning_rate': 0.01,\n","#  'batch_size': 16,\n","#  'best_avg_accuracy': 0.798898678414097}"],"metadata":{"id":"SdIPIsyTZILA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layers = [8]\n","\n","gru_N8_best_hyper_parameters = GRU_Hyper_Parameter_Tuning(layers, data,\n","                                                             time_step,\n","                                                             test_prop,\n","                                                             optimizers_names,\n","                                                             learning_rates,\n","                                                             batch_sizes,\n","                                                             epochs = epochs,\n","                                                             num_replicates=num_replicates)\n","gru_N8_best_hyper_parameters"],"metadata":{"id":"h_lTBpGwJDdS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# {'model': [8],\n","#  'optimizer': 'Nadam',\n","#  'learning_rate': 0.001,\n","#  'batch_size': 16,\n","#  'best_avg_accuracy': 0.8187613371339724}"],"metadata":{"id":"KsTqjgxzjKkp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layers = [16]\n","\n","gru_N16_best_hyper_parameters = GRU_Hyper_Parameter_Tuning(layers, data,\n","                                                             time_step,\n","                                                             test_prop,\n","                                                             optimizers_names,\n","                                                             learning_rates,\n","                                                             batch_sizes,\n","                                                             epochs = epochs,\n","                                                             num_replicates=num_replicates)\n","gru_N16_best_hyper_parameters"],"metadata":{"id":"edpaR1gqJDdS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# {'model': [16],\n","#  'optimizer': 'Adam',\n","#  'learning_rate': 0.001,\n","#  'batch_size': 16,\n","#  'best_avg_accuracy': 0.8414096916299559}"],"metadata":{"id":"lyQ2vJz9-EdS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layers = [32]\n","\n","gru_N32_best_hyper_parameters = GRU_Hyper_Parameter_Tuning(layers, data,\n","                                                             time_step,\n","                                                             test_prop,\n","                                                             optimizers_names,\n","                                                             learning_rates,\n","                                                             batch_sizes,\n","                                                             epochs = epochs,\n","                                                             num_replicates=num_replicates)\n","gru_N32_best_hyper_parameters"],"metadata":{"id":"4dtUMLeQJDdT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# {'model': [32],\n","#  'optimizer': 'Nadam',\n","#  'learning_rate': 0.001,\n","#  'batch_size': 32,\n","#  'best_avg_accuracy': 0.8633137542829173}"],"metadata":{"id":"v1bcJLy6VCes"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layers = [64]\n","\n","gru_N64_best_hyper_parameters = GRU_Hyper_Parameter_Tuning(layers, data,\n","                                                             time_step,\n","                                                             test_prop,\n","                                                             optimizers_names,\n","                                                             learning_rates,\n","                                                             batch_sizes,\n","                                                             epochs = epochs,\n","                                                             num_replicates=num_replicates)\n","gru_N64_best_hyper_parameters"],"metadata":{"id":"vqfadfssJDdT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# {'model': [64],\n","#  'optimizer': 'Adam',\n","#  'learning_rate': 0.01,\n","#  'batch_size': 8,\n","#  'best_avg_accuracy': 0.879295154185022}"],"metadata":{"id":"74Y-HWfXobDe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layers = [128]\n","\n","gru_N128_best_hyper_parameters = GRU_Hyper_Parameter_Tuning(layers, data,\n","                                                             time_step,\n","                                                             test_prop,\n","                                                             optimizers_names,\n","                                                             learning_rates,\n","                                                             batch_sizes,\n","                                                             epochs = epochs,\n","                                                             num_replicates=num_replicates)\n","gru_N128_best_hyper_parameters"],"metadata":{"id":"K_OyKP0eKYFQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# {'model': [128],\n","#  'optimizer': 'Nadam',\n","#  'learning_rate': 0.005,\n","#  'batch_size': 16,\n","#  'best_avg_accuracy': 0.8869100062932663}"],"metadata":{"id":"M36ewSBIKXox"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_LTYjxGKYrs"},"outputs":[],"source":["def GRU_Model(layers,\n","               hyper_parameters,\n","               data,\n","               time_step = 1,\n","               test_prop = 0.2,\n","               epochs = 5,\n","               num_replicates = 2):\n","\n","    X=data.drop(['business_cycle'],axis=1)\n","    y=np.array(data['business_cycle'])\n","\n","\n","    # Splitting train and test data\n","    X_train0, X_test0=sequential_split(X, test_prop)\n","    y_train0, y_test0=sequential_split(y, test_prop)\n","\n","    num_features = X_train0.shape[1]\n","\n","\n","    # Normalizing the train and test input using StandardScaler\n","    scaler = StandardScaler()\n","    X_train_scaled = scaler.fit_transform(X_train0).reshape(X_train0.shape[0],\n","                                                            num_features)\n","    X_test_scaled = scaler.transform(X_test0).reshape(X_test0.shape[0],\n","                                                      num_features)\n","\n","\n","    # Creating input data\n","    X_train,  y_train =DatasetCreation(X_train_scaled,y_train0, time_step)\n","    X_test,  y_test = DatasetCreation(X_test_scaled, y_test0, time_step)\n","\n","\n","    # arrays for collecting test scores\n","    accuracy_array = np.zeros(num_replicates)\n","    precision_array = np.zeros(num_replicates)\n","    recall_array    = np.zeros(num_replicates)\n","    f1score_array    = np.zeros(num_replicates)\n","    sensitivity_array    = np.zeros(num_replicates)\n","    specificity_array    = np.zeros(num_replicates)\n","    elapsed_time_array = np.zeros(num_replicates)\n","\n","\n","    models_history = []\n","    train_predictions = []\n","    test_predictions = []\n","\n","    for i in range(num_replicates):\n","\n","      print(\"Program is running for %d replicate ----->\\n\" %i)\n","\n","      model = Build_GRU_Model(layers, time_step, num_features,\n","                               optimizer = hyper_parameters[0],\n","                               learning_rate = hyper_parameters[1], verbose = 0)\n","\n","      callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience= 5)\n","      # This callback will stop the training when there is no improvement in\n","      # the loss for three consecutive epochs\n","      start = time.time()\n","\n","      history = model.fit(X_train, y_train, batch_size = hyper_parameters[2],\n","                          epochs= epochs, callbacks=[callback], verbose = 1)\n","\n","      end = time.time()\n","      elapsed_time = end - start\n","\n","      models_history.append(history)\n","\n","\n","      # Train and test prediction\n","      # Predict probabilities for each class\n","\n","      train_pred_probs = model.predict(X_train)\n","      # Threshold the probabilities to get class labels\n","      train_pred = (train_pred_probs > 0.5).astype(int)\n","\n","\n","      test_pred_probs = model.predict(X_test)\n","      # Threshold the probabilities to get class labels\n","      test_pred = (test_pred_probs > 0.5).astype(int)\n","\n","      # Compute evaluation metrics\n","      accuracy = accuracy_score(y_test, test_pred)\n","      precision = precision_score(y_test, test_pred)\n","      recall = recall_score(y_test, test_pred)\n","      f1 = f1_score(y_test, test_pred)\n","      tn, fp, fn, tp = confusion_matrix(y_test, test_pred).ravel()\n","      sensitivity = tp / (tp + fn)\n","      specificity = tn / (tn + fp)\n","\n","      train_predictions.append(train_pred)\n","      test_predictions.append(test_pred)\n","\n","\n","      accuracy_array[i] =  accuracy\n","      precision_array[i] = precision\n","      recall_array[i] = recall\n","      f1score_array[i] = f1\n","      sensitivity_array[i] = sensitivity\n","      specificity_array[i]=specificity\n","      elapsed_time_array[i] = elapsed_time\n","\n","    max_index = accuracy_array.argmax()\n","    best_accuracy = accuracy_array[max_index]\n","    precision_with_best_accuracy = precision_array[max_index]\n","    recall_with_best_accuracy =  recall_array[max_index]\n","    f1score_with_best_accuracy=f1score_array[max_index]\n","    sensitivity_with_best_accuracy=sensitivity_array[max_index]\n","    specificity_with_best_accuracy=specificity_array[max_index]\n","    elapsed_time_with_best_accuracy = elapsed_time_array[max_index]\n","\n","    train_predictions_with_best_accuracy = train_predictions[max_index]\n","    test_predictions_with_best_accuracy = test_predictions[max_index]\n","\n","    loss_with_best_accuracy = models_history[max_index].history['loss']\n","\n","    #val_loss_with_best_accuracy = models_history[max_index].history['val_loss']\n","\n","\n","    # Collecting important results\n","    performance_metrics =  {\n","\n","                        'scores': {'accuracy': accuracy_array,\n","                                    'precision': precision_array,\n","                                    'recall': recall_array,\n","                                    'f1':f1score_array,\n","                                   'sensitivity':sensitivity_array,\n","                                   'specificity':specificity_array,\n","                                    'elapsed_time': elapsed_time_array\n","                                    },\n","\n","                        'minimums': {'accuracy': np.min(accuracy_array),\n","                                      'precision': np.min(precision_array),\n","                                      'recall': np.min(recall_array),\n","                                      'f1': np.min(f1score_array),\n","                                      'sensitivity':np.min(sensitivity_array),\n","                                      'specificity': np.min(specificity_array),\n","                                      'elapsed_time': np.min(elapsed_time_array)\n","                                      },\n","\n","                        'avg_scores':  {'accuracy': np.mean(accuracy_array),\n","                                      'precision': np.mean(precision_array),\n","                                      'recall': np.mean(recall_array),\n","                                      'f1': np.mean(f1score_array),\n","                                      'sensitivity':np.mean(sensitivity_array),\n","                                      'specificity': np.mean(specificity_array),\n","                                      'elapsed_time': np.mean(elapsed_time_array)\n","                                      },\n","\n","                          'stds':     {'accuracy': np.std(accuracy_array),\n","                                      'precision': np.std(precision_array),\n","                                      'recall': np.std(recall_array),\n","                                      'f1': np.std(f1score_array),\n","                                      'sensitivity':np.std(sensitivity_array),\n","                                      'specificity': np.std(specificity_array),\n","                                      'elapsed_time': np.std(elapsed_time_array)\n","                                      },\n","\n","                        'maximums': {'accuracy': np.max(accuracy_array),\n","                                      'precision': np.max(precision_array),\n","                                      'recall': np.max(recall_array),\n","                                      'f1': np.max(f1score_array),\n","                                      'sensitivity':np.max(sensitivity_array),\n","                                      'specificity': np.max(specificity_array),\n","                                      'elapsed_time': np.max(elapsed_time_array)\n","                                      }\n","\n","                  }\n","\n","\n","\n","\n","    model_with_best_accuracy = {\n","\n","                            'replicate': max_index,\n","                            'accuracy': best_accuracy,\n","                            'precision': precision_with_best_accuracy,\n","                            'recall': recall_with_best_accuracy,\n","                            'f1': f1score_with_best_accuracy,\n","                            'sensitivity': sensitivity_with_best_accuracy,\n","                            'specificity': specificity_with_best_accuracy,\n","                            'elapsed_time': elapsed_time_with_best_accuracy,\n","                            'train_predictions':train_predictions_with_best_accuracy,\n","                            'test_predictions': test_predictions_with_best_accuracy,\n","                            'loss':loss_with_best_accuracy\n","                             #'val_loss': val_loss_with_best_accuracy\n","                            }\n","\n","     # Collecting hyperparameters\n","    hyper_parameters = {'layers': layers,\n","                        'model_specific_hyper_parameters': hyper_parameters,\n","                       'epochs': epochs,\n","                       'time_step':time_step,\n","                       'num_replicates': num_replicates,\n","                       'test_prop':test_prop\n","                        }\n","\n","\n","     #======= Collecting all the outputs together =============#\n","    output_dictionary = {'hyper_parameters': hyper_parameters,\n","                        'performance_metrics': performance_metrics,\n","                         'best_model': model_with_best_accuracy,\n","                       }\n","\n","    print(\"Progress: All works are done successfully, congratulations!!\\n\")\n","    return output_dictionary"]},{"cell_type":"code","source":[],"metadata":{"id":"x_ghONX4Kj4i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KprNLA5jK1N-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hyper_parameters= ['Adam', 0.001, 32]\n","layers=[8]\n","gru_output= GRU_Model(layers,\n","               hyper_parameters,\n","                        data,\n","               time_step = 1,\n","               test_prop = 0.3,\n","               epochs = 5,\n","               num_replicates = 2)\n","gru_output"],"metadata":{"id":"062v-4lOK1oz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N8Ggi3qNK1oz"},"source":["## **Executing Multiple GRU Models**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lskhtL1fK1oz"},"outputs":[],"source":["def Multiple_GRU_Models(hidden_layers,\n","                         hyper_parameters,\n","                         data,\n","                         time_step = 1,\n","                         test_prop = 0.2,\n","                         epochs = 5,\n","                         num_replicates = 2):\n","\n","  num_models = len(hidden_layers)\n","\n","  # collect all scores\n","  accuracy = []\n","  precision = []\n","  recall = []\n","  f1 = []\n","  sensitivity = []\n","  specificity = []\n","  elapsed_time = []\n","\n","  # collect all avg scores\n","  avg_accuracy = []\n","  avg_precision = []\n","  avg_recall = []\n","  avg_f1 = []\n","  avg_sensitivity = []\n","  avg_specificity = []\n","  avg_elapsed_time = []\n","\n","  # iteratively update the best rmse and the corresponding model\n","  best_accuracy = 0\n","  best_avg_accuracy = 0\n","  best_model_hidden_layers = None\n","  best_model_output = None\n","\n","  for i in range(num_models):\n","    #print(\"Running model with hidden neurons: \", hidden_layers[i])\n","\n","    #print(\"\\n\")\n","\n","    #print(\"Best Hyper_parameters used: \", hyper_parameters[i])\n","\n","    #print(\"\\n\")\n","\n","    output = GRU_Model(hidden_layers[i],\n","                        hyper_parameters[i],\n","                        data,\n","                        time_step,\n","                        test_prop,\n","                        epochs,\n","                        num_replicates)\n","\n","\n","    accuracy.append(output['performance_metrics']['scores']['accuracy'])\n","    precision.append(output['performance_metrics']['scores']['precision'])\n","    recall.append(output['performance_metrics']['scores']['recall'])\n","    f1.append(output['performance_metrics']['scores']['f1'])\n","    sensitivity.append(output['performance_metrics']['scores']['sensitivity'])\n","    specificity.append(output['performance_metrics']['scores']['specificity'])\n","    elapsed_time.append(output['performance_metrics']['scores']['elapsed_time'])\n","\n","    avg_accuracy.append(output['performance_metrics']['avg_scores']['accuracy'])\n","    avg_precision.append(output['performance_metrics']['avg_scores']['precision'])\n","    avg_recall.append(output['performance_metrics']['avg_scores']['recall'])\n","    avg_f1.append(output['performance_metrics']['avg_scores']['f1'])\n","    avg_sensitivity.append(output['performance_metrics']['avg_scores']['sensitivity'])\n","    avg_specificity.append(output['performance_metrics']['avg_scores']['specificity'])\n","    avg_elapsed_time.append(output['performance_metrics']['avg_scores']['elapsed_time'])\n","\n","    if avg_accuracy[i] > best_avg_accuracy:\n","      best_avg_accuracy = avg_accuracy[i]\n","      best_accuracy = output['best_model']['accuracy']\n","      best_model_hidden_layers = hidden_layers[i]\n","      best_model_output = output\n","\n","\n","  accuracy = np.array(accuracy)\n","  precision = np.array(precision)\n","  recall =  np.array(recall)\n","  f1 = np.array(f1)\n","  sensitivity = np.array(sensitivity)\n","  specificity =  np.array(specificity)\n","\n","  # Collecting all scores\n","\n","  performance_metrics = {\n","\n","       'scores':  {'layers': hidden_layers,\n","                   'accuracy': accuracy,\n","                   'precision': precision,\n","                   'recall':recall,\n","                   'f1': f1,\n","                   'sensitivity': sensitivity,\n","                   'specificity':specificity,\n","                   'elapsed_time': elapsed_time },\n","\n","       'avg_scores':  pd.DataFrame({'layers': hidden_layers,\n","                                    'accuracy': np.array(avg_accuracy),\n","                                    'precision': np.array(avg_precision),\n","                                    'recall':np.array(avg_recall),\n","                                    'f1': np.array(avg_f1),\n","                                    'sensitivity': np.array(avg_sensitivity),\n","                                    'specificity':np.array(avg_specificity),\n","                                    'elapsed_time':np.array(avg_elapsed_time)}),\n","\n","       'stds':     pd.DataFrame({'layers': hidden_layers,\n","                                 'accuracy': np.std(accuracy, axis = 1),\n","                                 'precision': np.std(precision, axis = 1),\n","                                 'recall':  np.std(recall, axis = 1 ),\n","                                 'f1': np.std(f1, axis = 1),\n","                                 'sensitivity': np.std(sensitivity, axis = 1),\n","                                 'specificity':  np.std(specificity, axis = 1 ),\n","                                 'elapsed_time': np.std(elapsed_time, axis = 1 )}),\n","\n","       'minimums': pd.DataFrame({'layers': hidden_layers,\n","                                'accuracy': np.min(accuracy, axis =1 ),\n","                                'precision': np.min(precision, axis= 1),\n","                                'recall': np.min(recall, axis =1),\n","                                'f1': np.min(f1, axis =1 ),\n","                                'sensitivity': np.min(sensitivity, axis= 1),\n","                                'specificity': np.min(specificity, axis =1),\n","                                'elapsed_time': np.min(elapsed_time, axis =1)}),\n","\n","       'maximums': pd.DataFrame({'layers': hidden_layers,\n","                                'accuracy': np.max(accuracy, axis =1),\n","                                'precision': np.max(precision, axis =1),\n","                                'recall': np.max(recall, axis =1),\n","                                'f1': np.max(f1, axis =1),\n","                                'sensitivity': np.max(sensitivity, axis =1),\n","                                'specificity': np.max(specificity, axis =1),\n","                                'elapsed_time': np.max(elapsed_time,axis =1)})\n","    }\n","\n","\n","  output_dictionary = {\n","                     'hyper_parameters': hyper_parameters,\n","\n","                      'best_avg_accuracy': best_avg_accuracy,\n","                      'best_accuracy': best_accuracy,\n","                      'best_model_hidden_layers': best_model_hidden_layers,\n","                      'best_model_output': best_model_output\n","                      }\n","\n","  #Save all statistics:\n","  performance_metrics['avg_scores'].to_csv(output_path+'multiple_lstm_models_average_scores.csv')\n","  performance_metrics['stds'].to_csv(output_path+'multiple_lstm_models_stds.csv')\n","  performance_metrics['minimums'].to_csv(output_path+'multiple_lstm_models_minimums.csv')\n","  performance_metrics['maximums'].to_csv(output_path+'multiple_lstm_models_maximums.csv')\n","\n","\n","  #Save all scores in the file for future analysis\n","  pd.DataFrame(performance_metrics['scores']['accuracy']).to_csv(output_path+'multiple_lstm_models_all_accuracy.csv')\n","  pd.DataFrame(performance_metrics['scores']['precision']).to_csv(output_path+'multiple_lstm_models_all_precision.csv')\n","  pd.DataFrame(performance_metrics['scores']['recall']).to_csv(output_path+'multiple_lstm_models_all_recall.csv')\n","  pd.DataFrame(performance_metrics['scores']['f1']).to_csv(output_path+'multiple_lstm_models_all_f1.csv')\n","  pd.DataFrame(performance_metrics['scores']['sensitivity']).to_csv(output_path+'multiple_lstm_models_all_sensitivity.csv')\n","  pd.DataFrame(performance_metrics['scores']['specificity']).to_csv(output_path+'multiple_lstm_models_all_specificity.csv')\n","\n","  #Save best model results\n","  pd.DataFrame(best_model_output['best_model']['loss']).to_csv(output_path+'best_lstm_model_loss.csv')\n","  pd.DataFrame(best_model_output['best_model']['train_predictions']).to_csv(output_path+'best_lstm_model_train_predictions.csv')\n","  pd.DataFrame(best_model_output['best_model']['test_predictions']).to_csv(output_path+'best_lstm_model_test_predictions.csv')\n","  pd.DataFrame(best_model_output['performance_metrics']['scores']['accuracy']).to_csv(output_path+'best_lstm_model_all_accuracy.csv')\n","\n","  #writing all result in the file\n","  write_dic_to_file(output_dictionary,\n","                    output_path + \"multiple_gru_models_full_results.txt\")\n","\n","  #Display some key results in the screen\n","  print(\"\\nBest model and its avg accuracy and maximum accuracy):\\n\", best_model_hidden_layers, best_avg_accuracy, best_accuracy)\n","  print(\"Hyper_parameters:\\n\", hyper_parameters)\n","  print('\\nAverage scores:\\n',  performance_metrics['avg_scores'])\n","  print('\\nStandard_deviations:\\n',  performance_metrics['stds'])\n","  print('\\nMinimums:\\n',  performance_metrics['minimums'])\n","  print('\\nMaximums:\\n',  performance_metrics['maximums'])\n","  print(\"Progress: All works are done successfully, congratulations!!\\n\")"]},{"cell_type":"markdown","metadata":{"id":"EuzCqIWUK1oz"},"source":["### **Execution**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEojTeJxK1o0"},"outputs":[],"source":["hidden_layers = [[4], [8], [16], [32],[64],[128]]\n","best_hyper_parameters   = [\n","                            ['Adagrad', 0.01, 16], #4N model\n","                            ['Nadam', 0.001, 16],  #8N model\n","                            ['Adam', 0.001, 16], #16N model\n","                            ['Nadam', 0.001, 32], #32N model\n","                            ['Adam', 0.01, 8], #64N model\n","                            ['Nadam', 0.005, 16] #64N model\n","                        ]\n","\n","Multiple_GRU_Models(hidden_layers,\n","                     best_hyper_parameters,\n","                     data,\n","                     time_step = 3,\n","                     test_prop = 0.3,\n","                     epochs = 30,\n","                     num_replicates = 30)"]},{"cell_type":"code","source":["# Average scores:\n","#    layers  accuracy  precision    recall        f1  sensitivity  specificity  \\\n","# 0    [4]  0.875184   0.940483  0.923833  0.929582     0.923833     0.371667\n","# 1    [8]  0.767988   0.975184  0.766506  0.853753     0.766506     0.783333\n","# 2   [16]  0.760206   0.954501  0.774235  0.851333     0.774235     0.615000\n","# 3   [32]  0.765345   0.943106  0.790499  0.858299     0.790499     0.505000\n","# 4   [64]  0.604405   0.942125  0.604026  0.732626     0.604026     0.608333\n","# 5  [128]  0.586050   0.922387  0.595974  0.721866     0.595974     0.483333\n","\n","#    elapsed_time\n","# 0      9.637760\n","# 1     11.699339\n","# 2     11.220967\n","# 3      8.512129\n","# 4     16.009488\n","# 5     12.927092\n","\n","# Standard_deviations:\n","#    layers  accuracy  precision    recall        f1  sensitivity  specificity  \\\n","# 0    [4]  0.051151   0.025895  0.074775  0.032903     0.074775     0.303503\n","# 1    [8]  0.087312   0.024501  0.103584  0.064410     0.103584     0.219975\n","# 2   [16]  0.086809   0.020980  0.098286  0.063870     0.098286     0.178022\n","# 3   [32]  0.060583   0.008765  0.071246  0.042052     0.071246     0.094296\n","# 4   [64]  0.068685   0.021430  0.081484  0.059494     0.081484     0.159208\n","# 5  [128]  0.060717   0.017243  0.068654  0.054826     0.068654     0.113529\n","\n","#    elapsed_time\n","# 0      2.223549\n","# 1      1.835493\n","# 2      1.945417\n","# 3      1.946306\n","# 4      5.717080\n","# 5      3.333227\n","\n","# Minimums:\n","#    layers  accuracy  precision    recall        f1  sensitivity  specificity  \\\n","# 0    [4]  0.726872   0.905660  0.700483  0.823864     0.700483         0.00\n","# 1    [8]  0.533040   0.923611  0.487923  0.655844     0.487923         0.25\n","# 2   [16]  0.537445   0.911111  0.502415  0.664537     0.502415         0.40\n","# 3   [32]  0.607930   0.929936  0.608696  0.739003     0.608696         0.35\n","# 4   [64]  0.431718   0.913386  0.391304  0.556701     0.391304         0.45\n","# 5  [128]  0.387665   0.886364  0.376812  0.528814     0.376812         0.35\n","\n","#    elapsed_time\n","# 0      6.834773\n","# 1      8.557289\n","# 2      8.331985\n","# 3      6.382083\n","# 4      7.701796\n","# 5      6.795640\n","\n","# Maximums:\n","#    layers  accuracy  precision    recall        f1  sensitivity  specificity  \\\n","# 0    [4]  0.955947   1.000000  1.000000  0.976190     1.000000         1.00\n","# 1    [8]  0.933921   1.000000  1.000000  0.965035     1.000000         1.00\n","# 2   [16]  0.885463   0.993711  0.932367  0.936893     0.932367         0.95\n","# 3   [32]  0.872247   0.967949  0.917874  0.929095     0.917874         0.75\n","# 4   [64]  0.814978   1.000000  0.850242  0.893401     0.850242         1.00\n","# 5  [128]  0.726872   0.991803  0.763285  0.835979     0.763285         0.95\n","\n","#    elapsed_time\n","# 0     13.103123\n","# 1     13.695225\n","# 2     13.808754\n","# 3     18.143008\n","# 4     23.755467\n","# 5     23.762189\n","# Progress: All works are done successfully, congratulations!!"],"metadata":{"id":"EdOP9fZzLZ2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SDtwDmo1E8as"},"execution_count":null,"outputs":[]}]}